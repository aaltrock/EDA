clear();
addpath(genpath('.'));

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONFIGURATIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Configuration - percentage of population for validation and test
% Note: training size is the remainder of validation and test
val_pct = 0.15;
tst_pct = 0.15;

% Configuration - feed forward hidden layers configuration
ff_hidden_layers = [2]; % Vector of neuro numbers per hidden layer
ff_trn_func = "trainbr"; % Training function
max_epoch = 50; % Max no. of epoch

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DATA IMPORT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Read in the pre-processed file generated by Python code
fungi_src = readtable('src_df.xlsx');
% fungi_src = table2array(fungi_src);

% Extract the independent variables
src_x = fungi_src(:, 2:end);

% Extract the dependent variable
% (specifically 'Edible' variable only as the target)
src_y = fungi_src(:, 1);

% Convert from tables to matrix
src_x = table2array(src_x);
src_y = table2array(src_y);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SIZES FOR TRAINING, VALIDATION & TESTING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Determine size of the training, validation and test set
pop_size = size(fungi_src, 1);

disp("Population size:");
disp(pop_size);

val_size = round(val_pct * pop_size);
disp("Validation size:");
disp(val_size);

tst_size = round(tst_pct * pop_size);
disp("Test size:");
disp(tst_size);

% Training set size is the remainder of population after validation and
% test
trn_size = pop_size - val_size - tst_size;
disp("Training size:");
disp(trn_size);

% Validate the total size corresponds to original population size
disp("Training + Validation + Test size:");
disp(trn_size + val_size + tst_size);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RANDOM SELECTIONS FOR TESTING, VALIDATION & TRAINING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Randomly select from index for test per calculated size
tst_idx = randsample(pop_size,tst_size);

% Remove samples selected for testing
trn_val_idx = [];
for i = 1:pop_size
    % If sample not selected in test, add to training and validation set
    if ~ismember(i, tst_idx)
        trn_val_idx = [trn_val_idx; i];
    end
end

% Randomly select from remainder for validation per calculated size
trn_val_rand_idx = randsample(size(trn_val_idx, 1), tst_size);

% Compile validation samples set
val_idx = [];
for i = trn_val_rand_idx
    % Add validation set samples selected randomly by index
    val_idx = [val_idx; trn_val_idx(i)];
end

% Compile training samples set by remainder after test and validation
% selections
trn_idx = [];
for i = 1:size(trn_val_idx,1)
    % If sample not selected in validation, add to training
    if ~ismember(trn_val_idx(i), val_idx)
        trn_idx = [trn_idx; trn_val_idx(i)];
    end
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COMPILE TRAINING, VALIDATION & TEST SETS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Compile training set
trn_x = create_nn_sample_set(trn_idx, src_x);
trn_y = create_nn_sample_set(trn_idx, src_y);
trn_xy = [trn_x trn_y];

% Compile validation set
val_x = create_nn_sample_set(val_idx, src_x);
val_y = create_nn_sample_set(val_idx, src_y);

% Compile test set
tst_x = create_nn_sample_set(tst_idx, src_x);
tst_y = create_nn_sample_set(tst_idx, src_y);



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRAINING FEEDFORWARD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Construct a feed forward neural network
ff_net_resampled = fitnet(ff_hidden_layers, ff_trn_func);
view(ff_net_resampled);

% Configure neural network
ff_net_resampled.trainParam.epochs=max_epoch; % Epoch

% Train network with re-sampled data
ff_net_resampled = train(ff_net_resampled, transpose(trn_x), transpose(trn_y));
disp(ff_net_resampled.trainParam);


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EVALUATE WITH VALIDATION SET
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Predict with validation set
val_p = table(transpose(ff_net_resampled(transpose(val_x))));

% Compare predicted with actuals
val_p.Properties.VariableNames = {'Predicted'};
val_p.Actual = val_y;
val_p.Variance_Exact = val_p.Predicted - val_p.Actual;
val_p.Variance_Class = round(val_p.Predicted) - val_p.Actual;


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EVALUATE WITH TEST SET
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Predict with test set
tst_p = table(transpose(ff_net_resampled(transpose(tst_x))));

% Compare predicted with actuals
tst_p.Properties.VariableNames = {'Predicted'};
tst_p.Actual = tst_y;
tst_p.Variance_Exact = tst_p.Predicted - tst_p.Actual;
tst_p.Variance_Class = round(tst_p.Predicted) - tst_p.Actual;




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESAMPLE TO BALANCE MINORITY CLASSES IN TARGET VARIABLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % Apply SMOTE to balance minor classes
% [trn_x_SMOTE, trn_y_SMOTE] = SMOTE(trn_x, trn_y);
% 
% % In training set, count the frequency per class in the target variable
% [grp_cnt, grp_class] = groupcounts(trn_y);
% trn_y_cnt = [grp_class, grp_cnt];
% grp_cnt_pct = trn_y_cnt(:,2) ./ sum(trn_y_cnt(:,2)) .* 100; %Frequency per closed code
% grp_cnt_rank_idc = floor(tiedrank(trn_y_cnt(:,2))); %Rank incdices of count ascending
% trn_y_cnt = [trn_y_cnt, grp_cnt_pct, grp_cnt_rank_idc];
% 
% % In training set, count the frequency per class in the target variable
% [grp_SMOTE_cnt, grp_SMOTE_class] = groupcounts(trn_y_SMOTE);
% trn_y_SMOTE_cnt = [grp_SMOTE_class, grp_SMOTE_cnt];
% grp_cnt_SMOTE_pct = trn_y_SMOTE_cnt(:,2) ./ sum(trn_y_SMOTE_cnt(:,2)) .* 100; %Frequency per closed code
% grp_cnt_rank_SMOTE_idc = floor(tiedrank(trn_y_SMOTE_cnt(:,2))); %Rank incdices of count ascending
% trn_y_SMOTE_cnt = [trn_y_SMOTE_cnt, grp_cnt_SMOTE_pct, grp_cnt_rank_SMOTE_idc];
% 

% % In training set, count the frequency per class in the target variable
% [grp_cnt, grp_class] = groupcounts(trn_y);
% trn_y_cnt = [grp_class, grp_cnt];
% grp_cnt_pct = trn_y_cnt(:,2) ./ sum(trn_y_cnt(:,2)) .* 100; %Frequency per closed code
% grp_cnt_rank_idc = floor(tiedrank(trn_y_cnt(:,2))); %Rank incdices of count ascending
% trn_y_cnt = [trn_y_cnt, grp_cnt_pct, grp_cnt_rank_idc];
% 
% disp('No. of classes:');
% disp(size(trn_y_cnt,1));
% 
% % Calculate mean class size
% class_mean = floor(mean(trn_y_cnt(:,2)));
% 
% % Re-sample by adjusting the class size in training set to mean class size
% trn_xy_resampled = [];
% for i = 1:size(trn_y_cnt,1)
%     disp(i);
%     class_val = trn_y_cnt(i,1);
%     new_trn_xy = adjust_samples(class_val, trn_xy, class_mean);
%     trn_xy_resampled = [trn_xy_resampled; new_trn_xy];
% end
% 
% % Split training set to dependent and independent variables
% trn_x_resampled = trn_xy_resampled(:,1:size(trn_xy_resampled,2)-1);
% trn_y_resampled = trn_xy_resampled(:,size(trn_xy_resampled,2));
% 
% 
% % In training set, re-count the frequency per class in re-sampled training set
% [grp_cnt_resampled, grp_class_resampled] = groupcounts(trn_y_resampled);
% trn_y_cnt_resampled = [grp_class_resampled, grp_cnt_resampled];
% grp_cnt_resampled_pct = trn_y_cnt_resampled(:,2) ./ sum(trn_y_cnt_resampled(:,2)) .* 100; %Frequency per closed code
% grp_cnt_rank_resampled_idc = floor(tiedrank(trn_y_cnt_resampled(:,2))); %Rank incdices of count ascending
% trn_y_cnt_resampled = [trn_y_cnt_resampled, grp_cnt_resampled_pct, grp_cnt_rank_resampled_idc];



% % Evaluate with validation set
% val_perf = perform(ff_net_resampled, val_x, val_y);
% disp(ff_net_resampled.performFcn);
% fprintf("MSE: %d\n", val_perf);

% Function to over or under sample based on mean class size
function new_sample_m = adjust_samples(class_val, trn_set, mean_size)
    % Index of column with the target values
    target_col_idx = size(trn_set, 2);
    
    % Extract rows with the class value
    trn_class_set = [];
    for i = 1:size(trn_set, 1)
        if trn_set(i, target_col_idx) == class_val
            trn_class_set = [trn_class_set; trn_set(i, :)];
        end
    end
    fprintf("Mean class size: %d\n", mean_size);
    fprintf("Class value: %d\n", class_val);
    fprintf("Class size: %d\n", size(trn_class_set, 1));
    
    % Check if class size is over or below mean size of all classes
    if size(trn_class_set, 1) < mean_size
        disp("To over-sample...");
        % Over-sample - generate random number up to class size to mean
        % size vector
        new_sample_idx = randi(size(trn_class_set, 1), mean_size, 1);
        new_sample_m = [];
        % Per randomly selected index, compile a new training set
        for i = new_sample_idx
            new_sample_m = [new_sample_m; trn_class_set(i, :)];
        end
        fprintf("Re-sampled class size: %d\n", size(new_sample_m,1));
    elseif size(trn_class_set, 1) > mean_size
        disp("To under-sample...");
        % Under-sample to mean size
        new_sample_idx = randsample(size(trn_class_set, 1), mean_size);
        new_sample_m = [];
        % Per randomly selected index, compile a new training set
        for i = new_sample_idx
            new_sample_m = [new_sample_m; trn_class_set(i, :)];
        end
        fprintf("Re-sampled class size: %d\n", size(new_sample_m,1));
    else
        disp("No re-sampling...");
        % Class size matches mean exactly, return as is
        new_sample_m = trn_set;
    end
    
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEURAL NETWORK TRAINING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% conf.hidNum = [20, 30]; % number of nodes in each hidden layer
% conf.activationFnc = {'tansig','logsig','tansig'}; % tansig, logsig, purelin
% conf.eNum   = 10;% set number of epochs
% conf.bNum   = 0; % set later in each dataset
% conf.sNum   = 0; % set later in each dataset
% conf.params = [0.5 0.1 0.1 0.0001];
% 
% conf.E_STOP = 10;
% 
% disp("____________________________________________");
% disp("     NETWORK TRAINING");
% disp("____________________________________________");
% model = train_nn(conf, [], [], trn_x, trn_y, val_x, val_y);
% 
% 
% 




% 
% 
% 
% function model = train_nn(conf,Ws,bs,trn_dat,trn_lab,vld_dat,vld_lab)
% 
% [SZ,visNum] = size(trn_dat);
% depth = length(conf.hidNum)+1;
% labNum = size(unique(trn_lab),1);
% if isempty(Ws)
%  % Initialize Ws
%     model.Ws{1} = (1/visNum)*(2*rand(visNum,conf.hidNum(1))-1);
%     DW{1} = zeros(size(model.Ws{1}));
%     for i=2:depth-1
%         model.Ws{i} = (1/conf.hidNum(i-1))*(2*rand(conf.hidNum(i-1),conf.hidNum(i))-1);
%         DW{i} = zeros(size(model.Ws{i}));
%     end
%     model.Ws{depth} = (1/conf.hidNum(depth-1))*(2*rand(conf.hidNum(depth-1),labNum)-1);
%     DW{depth} = zeros(size(model.Ws{depth}));
% else
%     model.Ws = Ws;
%     for i=1:depth, DW{i} = zeros(size(model.Ws{i})); size(model.Ws{i}); end
%     clear Ws
% end
% 
% if isempty(bs)
%  % Initialize bs
%     for i=1:depth-1
%         model.bs{i} = zeros(1,conf.hidNum(i));
%         DB{i} = model.bs{i};
%     end
%     model.bs{depth} = zeros(1,labNum);
%     DB{depth} = model.bs{depth};
% else 
%     model.bs  = bs;
% end
% bNum = conf.bNum;
% if conf.bNum == 0, bNum   = round(SZ/conf.sNum); end
% 
% plot_trn_acc = [];
% plot_vld_acc = [];
% plot_mse = [];
% 
% vld_best = 0;
% es_count = 0;
% acc_drop_count = 0;
% vld_acc  = 0;
% tst_acc  = 0;
% e = 0;
% running =1;
% 
% lr = conf.params(1);
% while running
%     MSE = 0;
%     e = e+1;
%    for b=1:bNum
%        inx = (b-1)*conf.sNum+1:min(b*conf.sNum,SZ);
%        batch_x = trn_dat(inx,:);
%        batch_y = trn_lab(inx)+1;
%        sNum = size(batch_x,1);
%        % Forward mesage to get output
%        input{1} = bsxfun(@plus,batch_x*model.Ws{1},model.bs{1});
%        actFunc=  str2func(conf.activationFnc{1});
%        output{1} = actFunc(input{1});       
%        for i=2:depth
%            input{i} = bsxfun(@plus,output{i-1}*model.Ws{i},model.bs{i});
%            actFunc=  str2func(conf.activationFnc{i});
%            output{i} = actFunc(input{i});
%        end  
%        %output{depth} = output{depth}
%        % Back-prop update        
%        y = discrete2softmax(batch_y,labNum);
%        %disp([y output{depth}]);
%        
%        err{depth} = (y-output{depth}).*deriv(conf.activationFnc{depth},input{depth});
%        %err
%        [~,cout] = max(output{depth},[],2);
%        %sum(sum(batch_y+1==cout))
%        MSE = MSE + mean(sqrt(mean((output{depth}-y).^2)));
%        for i=depth:-1:2
%            diff = output{i-1}'*err{i}/sNum;
%            DW{i} = lr*(diff - conf.params(4)*model.Ws{i}) + conf.params(3)*DW{i};
%            model.Ws{i} = model.Ws{i} + DW{i};
%        
%            DB{i} = lr*mean(err{i}) + conf.params(3)*DB{i};
%            model.bs{i} = model.bs{i} + DB{i};
%            err{i-1} = err{i}*model.Ws{i}'.*deriv(conf.activationFnc{i},input{i-1});
%        end
%        diff = batch_x'*err{1}/sNum;        
%        DW{1} = lr*(diff - conf.params(4)*model.Ws{1}) + conf.params(3)*DW{1};
%        model.Ws{1} = model.Ws{1} + DW{1};       
%        
%        DB{1} = lr*mean(err{1}) + conf.params(3)*DB{1};
%        model.bs{1} = model.bs{1} + DB{1};       
%    end
%    
%    % Get training classification error
%    %trn_dat
%    %model.Ws{1} = 0*model.Ws{1};
%    %model.Ws{2} = 0*model.Ws{2};
%    %pause
%    cout = run_nn(conf.activationFnc,model,trn_dat); 
%    %cout
%    trn_acc = sum((cout-1)==trn_lab)/size(trn_lab,1);
%    cout = run_nn(conf.activationFnc,model,vld_dat);
%    vld_acc = sum((cout-1)==vld_lab)/size(vld_lab,1);
%    fprintf('[Eppoch %4d] MSE = %.5f| Train acc = %.5f|Validation = %.5f\n',e,MSE,trn_acc,vld_acc);
%    % Collect data for plot
%    
%    plot_trn_acc = [plot_trn_acc trn_acc];
%    plot_vld_acc = [plot_vld_acc vld_acc];
%    plot_mse     = [plot_mse MSE];
%    %pause;
%    
%    %% EARLY STOPPING
%    % PARAM DECAY
%     if isfield(conf,'E_STOP_LR_REDUCE')
%         if vld_acc<=vld_best
%             acc_drop_count = acc_drop_count + 1;
%             % If accuracy reduces for a number of time, then turn back to the
%             % best model and reduce the learning rate
%             if acc_drop_count > conf.E_STOP_LR_REDUCE
%                 fprintf('Learning rate reduced!\n');
%                 acc_drop_count = 0;
%                 es_count = es_count + 1; %number of reduce learning rate
%                 lr = lr/10;
%                 model = model_best;
%             end
%         else
%             es_count = 0;
%             acc_drop_count = 0;
%             vld_best = vld_acc;
%             tst_best = tst_acc;
%             model_best = model;
%         end
%     end
%     % Early stopping
%     if isfield(conf,'E_STOP') 
%         if isfield(conf,'desire_acc') && vld_acc >= conf.desire_acc, running=0;end
%         if es_count > conf.E_STOP, running=0; end
%     end
% 
%     % Check stop
%     if e>=conf.eNum, running=0; end
%     
% end
%     fig1 = figure(1);
%     set(fig1,'Position',[10,20,300,200]);
%     plot(1:size(plot_trn_acc,2),plot_trn_acc,'r');
%     hold on;
%     plot(1:size(plot_vld_acc,2),plot_vld_acc);    
%     legend('Training','Validation');
%     xlabel('Epochs');ylabel('Accuracy');
%     
%     fig2 = figure(2);
%     set(fig2,'Position',[10,20,300,200]);
%     plot(1:size(plot_mse,2),plot_mse);    
%     xlabel('Epochs');ylabel('MSE');
% end




function nn_lab()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exprimental code for Neural Networks                  %
% The code is developed for Neural Computing tutorial   %
% MSc Data Science, CITY UNIVERSITY LONDON              %
% Authors: Son Tran, Artur Garcez                       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
addpath(genpath('.'));
conf.hidNum = [20, 30]; % number of nodes in each hidden layer
conf.activationFnc = {'tansig','logsig','tansig'}; % tansig, logsig, purelin
conf.eNum   = 10;% epoch number
conf.bNum   = 10; % batch number
conf.sNum   = round(trn_size/10); % sample size per batch
conf.params = [0.5 0.1 0.1 0.0001];

conf.E_STOP = 10;

model = train_nn(conf,[],[],trn_dat,trn_lab,vld_dat,vld_lab);


if CASE==1
    load glass_dataset;
    all_dat = glassInputs';
    all_dat = bsxfun(@rdivide,bsxfun(@minus,all_dat,min(all_dat)),(max(all_dat)-min(all_dat)));
    [~,all_lab] = max(glassTargets',[],2); all_lab = all_lab-1;
    trn_dat = [];trn_lab = [];vld_dat = [];vld_lab=[];
    for l=unique(all_lab)'
        inx= find(all_lab==l);
        trn_num = round(0.7*size(inx,1)); % get 60% of this label for training
        trn_dat = [trn_dat;all_dat(inx(1:trn_num),:)];
        vld_dat = [vld_dat;all_dat(inx(trn_num+1:end),:)];
        
        trn_lab = [trn_lab;all_lab(inx(1:trn_num))];
        vld_lab = [vld_lab;all_lab(inx(trn_num+1:end))];
    end
    
    conf.bNum = 1; conf.sNum = size(trn_dat,1);    
elseif CASE==2
    conf.bNum = 50;conf.sNum = 100; %number of batches and number of samples in each batch    
    trn_dat_file = 'mnist_train_dat_5k.mat';
    trn_lab_file = 'mnist_train_lab_5k.mat';
    vld_dat_file = 'mnist_vld_dat_10k.mat';
    vld_lab_file = 'mnist_vld_lab_10k.mat';
    tst_dat_file = 'mnist_test_dat_10k.mat';
    tst_lab_file = 'mnist_test_lab_10k.mat';
    
    trn_dat = get_data_from_file(trn_dat_file);
    trn_lab = get_data_from_file(trn_lab_file);
    vld_dat = get_data_from_file(vld_dat_file);
    vld_lab = get_data_from_file(vld_lab_file);
    tst_dat = get_data_from_file(tst_dat_file);
    tst_lab = get_data_from_file(tst_lab_file);
    show_images(trn_dat,100,28,28);
end

model = train_nn(conf,[],[],trn_dat,trn_lab,vld_dat,vld_lab);

if CASE==2, visualize_1l_filters(model.Ws{1},10,28,28,'minmax'); end
disp(model.Ws{1});
disp(model.Ws{2});
disp(model.Ws{3});

if exist('tst_dat','var')
   cout = run_nn(conf.activationFnc,model,tst_dat);
   tst_acc = sum((cout-1)==tst_lab)/size(tst_lab,1);
   fprintf('Test accuracy = %.5f\n',tst_acc);
end
end


